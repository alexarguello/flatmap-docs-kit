"use strict";(self.webpackChunkdocusaurus_resource=self.webpackChunkdocusaurus_resource||[]).push([[429],{5785:(e,i,t)=>{t.r(i),t.d(i,{assets:()=>l,contentTitle:()=>a,default:()=>u,frontMatter:()=>o,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"unmet-needs/unmet-needs","title":"Unmet Needs & Future Possibilities","description":"- Advanced reading workflows for scientists (skimming, referencing, jumping between sections). Currently, OCR and screen readers are limited to reading text in graphs. True accessibility for statistical graphs requires authors to include descriptive text and, ideally, provide underlying data in accessible formats. AI solutions for automatic graph interpretation are in early stages and not yet reliable for critical scientific content.","source":"@site/docs/70-unmet-needs/unmet-needs.md","sourceDirName":"70-unmet-needs","slug":"/unmet-needs/unmet-needs","permalink":"/accessibility-hub/docs/unmet-needs/unmet-needs","draft":false,"unlisted":false,"editUrl":"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/70-unmet-needs/unmet-needs.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"title":"Unmet Needs & Future Possibilities","sidebar_position":1,"hide_title":true,"level":"intermediate","type":"overview","status":"draft","visibility":"public","topics":["unmet-needs","future","accessibility","ai","tool:ocr","tool:vision-ai","user:scientist","user:disabled"],"author":["Alexandra Arguello Saenz (https://github.com/alexarguello)"],"eta":"2024-12-06T00:00:00.000Z"},"sidebar":"tutorialSidebar","previous":{"title":"Unmet Needs","permalink":"/accessibility-hub/docs/unmet-needs/"},"next":{"title":"Contribute","permalink":"/accessibility-hub/docs/contribute/"}}');var n=t(4848),r=t(8453);const o={title:"Unmet Needs & Future Possibilities",sidebar_position:1,hide_title:!0,level:"intermediate",type:"overview",status:"draft",visibility:"public",topics:["unmet-needs","future","accessibility","ai","tool:ocr","tool:vision-ai","user:scientist","user:disabled"],author:["Alexandra Arguello Saenz (https://github.com/alexarguello)"],eta:new Date("2024-12-06T00:00:00.000Z")},a="Unmet Needs & Future Possibilities",l={},c=[];function d(e){const i={h1:"h1",header:"header",li:"li",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(i.header,{children:(0,n.jsx)(i.h1,{id:"unmet-needs--future-possibilities",children:"Unmet Needs & Future Possibilities"})}),"\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Advanced reading workflows"})," for scientists (skimming, referencing, jumping between sections). Currently, OCR and screen readers are limited to reading text in graphs. True accessibility for statistical graphs requires authors to include descriptive text and, ideally, provide underlying data in accessible formats. AI solutions for automatic graph interpretation are in early stages and not yet reliable for critical scientific content."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"AI for daily tasks"}),": Cooking, matching outfits, organizing spaces."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Assistive GPS/navigation"})," for indoor environments (hotels, offices)."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Better video content description"}),": AI describes visual cues (\u201cclick here\u201d, \u201cgo there\u201d)."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Social support and peer connection"})," for people newly experiencing disability."]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Creating Accessible Training Content"})," :","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsxs)(i.li,{children:["Co-developed materials for people with disabilities to learn:","\n",(0,n.jsxs)(i.ul,{children:["\n",(0,n.jsx)(i.li,{children:"Accessibility features in Windows, Word, Excel, etc."}),"\n",(0,n.jsx)(i.li,{children:"Using Copilot tailored for different disabilities (visual, hearing, cognitive)."}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(i.li,{children:[(0,n.jsx)(i.strong,{children:"Goal"}),": Empower users to increase productivity through adaptive AI."]}),"\n"]}),"\n"]}),"\n"]})]})}function u(e={}){const{wrapper:i}={...(0,r.R)(),...e.components};return i?(0,n.jsx)(i,{...e,children:(0,n.jsx)(d,{...e})}):d(e)}},8453:(e,i,t)=>{t.d(i,{R:()=>o,x:()=>a});var s=t(6540);const n={},r=s.createContext(n);function o(e){const i=s.useContext(r);return s.useMemo(function(){return"function"==typeof e?e(i):{...i,...e}},[i,e])}function a(e){let i;return i=e.disableParentContext?"function"==typeof e.components?e.components(n):e.components||n:o(e.components),s.createElement(r.Provider,{value:i},e.children)}}}]);